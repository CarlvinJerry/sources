<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Mining on Beyond Raw Data</title>
    <link>/categories/data-mining/</link>
    <description>Recent content in Data Mining on Beyond Raw Data</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 17 Feb 2019 00:00:00 +0000</lastBuildDate>
    
        <atom:link href="https://beyondrawdata.rbind.io/categories/data-mining/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>DIGITAL NUDGING</title>
      <link>/post/2019-02-20-digital-nudging-rmd/</link>
      <pubDate>Sun, 17 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-02-20-digital-nudging-rmd/</guid>
      <description>


&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction:&lt;/h3&gt;
&lt;p&gt;Here is a fun fact; An average human being (probably an adult) makes close to 30,000 conscious decisions every day. This isn’t entirely true though, in fact, I just made that number up. I could be right because if you think about it, how many decisions would you say you make on a day to day basis? Depending on who you are the above obviously varies widely and you know best. We all make &lt;strong&gt;&lt;strong&gt;n&lt;/strong&gt;&lt;/strong&gt; decisions every day- what to do, eat, buy or hit. The real question however is, do we always choose well? Are there any other factors at hand that influence our decision making process? Are all these factors, if any, always straight forward choices or do we sometimes get &lt;strong&gt;“nudged”&lt;/strong&gt; into these choices we make?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Nudge theory&lt;/strong&gt; basically states that; by understanding how people think and what drives their decisions, we can use those factors to steer them into making decisions differently, through positive reinforcement. Research has shown that, by presenting choices differently rather than in a legislative manner, people can be influenced into making specific desired choices. This theory is widely used in behavioral economics by presenting subtle &lt;strong&gt;nudge units&lt;/strong&gt; intended to influence people’s thoughts about financial products. The theory was however initially more of a moral aspect meant to help people make better decisions in life and not as a tool for commercial gain. Over years of practice, different applications of the theory emerged.&lt;/p&gt;
&lt;p&gt;Now that we have a basic understanding of what nudge theory is about, we can explore an applicable example. This post mainly focuses on a short research project I happened to be part of, actually my first hackathon experience hosted by Safaricom PLC. Let’s dive in!&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-challenge&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;The Challenge&lt;/h3&gt;
&lt;p&gt;This photo a team mate took at the hackathon contains a problem statement for the challenge:&lt;/p&gt;
&lt;/center&gt;
&lt;figure&gt;
&lt;img class=&#34;pure-u-1-2&#34; src=&#34;https://github.com/CarlvinJerry/CarlvinJerry.github.io/blob/master/post/MyImages/theChallenge.jpg?raw=true&#34; height=&#34;100%&#34; width=&#34;Auto&#34;&gt;
&lt;figcaption&gt;
Figure 1: The challenge
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;div id=&#34;tools-used&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Tools used:&lt;/h4&gt;
&lt;p&gt;Our twitter data was fetched using &lt;strong&gt;R&lt;/strong&gt;, I have done a post on setting up a twitter API to fetch twitter data &lt;a href=&#34;https://beyondrawdata.rbind.io/2019/01/25/data-mining-in-r/&#34;&gt;here&lt;/a&gt;. &lt;strong&gt;R&lt;/strong&gt; has several packages (such as &lt;strong&gt;“tweeteR”&lt;/strong&gt; and &lt;strong&gt;“rtweet”&lt;/strong&gt;) that one can use to stream data from twitter. Our data cleaning and pre-processing was mainly done in &lt;strong&gt;Python&lt;/strong&gt;.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; To keep this post concise, code for the workings has been minimized. &lt;a href=&#34;https://github.com/CarlvinJerry/sources/blob/master/content/post/2019-02-20-digital-nudging-rmd.Rmd&#34;&gt;The source code for this post can be found here,&lt;/a&gt; for anyone interested in trying out the same process. The code is well commented for easier understanding as well.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div id=&#34;fetching-data&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;1.Fetching Data&lt;/h4&gt;
&lt;p&gt;The team agreed on a few terms to query data on from twitter. For an unbiased range of topics, we settled on fetching tweets under trending topics and a few more from random words. We had tweets from or containing the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;#MenConference2019&lt;/strong&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“Here”&lt;/strong&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;#r_Stats&lt;/strong&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;“PWC”&lt;/strong&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;#Friday Feeling&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A total of &lt;strong&gt;7000 tweets&lt;/strong&gt; were captured. The data frame had a total of 88 columns which we treated as variables for the research. However, not all variables were used in the research we therefore had to do some data cleaning. Here is a preview of the variables in our raw data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; [1] &amp;quot;user_id&amp;quot;                 &amp;quot;status_id&amp;quot;               &amp;quot;created_at&amp;quot;             
 [4] &amp;quot;screen_name&amp;quot;             &amp;quot;text&amp;quot;                    &amp;quot;source&amp;quot;                 
 [7] &amp;quot;display_text_width&amp;quot;      &amp;quot;reply_to_status_id&amp;quot;      &amp;quot;reply_to_user_id&amp;quot;       
[10] &amp;quot;reply_to_screen_name&amp;quot;    &amp;quot;is_quote&amp;quot;                &amp;quot;is_retweet&amp;quot;             
[13] &amp;quot;favorite_count&amp;quot;          &amp;quot;retweet_count&amp;quot;           &amp;quot;hashtags&amp;quot;               
[16] &amp;quot;symbols&amp;quot;                 &amp;quot;urls_url&amp;quot;                &amp;quot;urls_t.co&amp;quot;              
[19] &amp;quot;urls_expanded_url&amp;quot;       &amp;quot;media_url&amp;quot;               &amp;quot;media_t.co&amp;quot;             
[22] &amp;quot;media_expanded_url&amp;quot;      &amp;quot;media_type&amp;quot;              &amp;quot;ext_media_url&amp;quot;          
[25] &amp;quot;ext_media_t.co&amp;quot;          &amp;quot;ext_media_expanded_url&amp;quot;  &amp;quot;ext_media_type&amp;quot;         
[28] &amp;quot;mentions_user_id&amp;quot;        &amp;quot;mentions_screen_name&amp;quot;    &amp;quot;lang&amp;quot;                   
[31] &amp;quot;quoted_status_id&amp;quot;        &amp;quot;quoted_text&amp;quot;             &amp;quot;quoted_created_at&amp;quot;      
[34] &amp;quot;quoted_source&amp;quot;           &amp;quot;quoted_favorite_count&amp;quot;   &amp;quot;quoted_retweet_count&amp;quot;   
[37] &amp;quot;quoted_user_id&amp;quot;          &amp;quot;quoted_screen_name&amp;quot;      &amp;quot;quoted_name&amp;quot;            
[40] &amp;quot;quoted_followers_count&amp;quot;  &amp;quot;quoted_friends_count&amp;quot;    &amp;quot;quoted_statuses_count&amp;quot;  
[43] &amp;quot;quoted_location&amp;quot;         &amp;quot;quoted_description&amp;quot;      &amp;quot;quoted_verified&amp;quot;        
[46] &amp;quot;retweet_status_id&amp;quot;       &amp;quot;retweet_text&amp;quot;            &amp;quot;retweet_created_at&amp;quot;     
[49] &amp;quot;retweet_source&amp;quot;          &amp;quot;retweet_favorite_count&amp;quot;  &amp;quot;retweet_retweet_count&amp;quot;  
[52] &amp;quot;retweet_user_id&amp;quot;         &amp;quot;retweet_screen_name&amp;quot;     &amp;quot;retweet_name&amp;quot;           
[55] &amp;quot;retweet_followers_count&amp;quot; &amp;quot;retweet_friends_count&amp;quot;   &amp;quot;retweet_statuses_count&amp;quot; 
[58] &amp;quot;retweet_location&amp;quot;        &amp;quot;retweet_description&amp;quot;     &amp;quot;retweet_verified&amp;quot;       
[61] &amp;quot;place_url&amp;quot;               &amp;quot;place_name&amp;quot;              &amp;quot;place_full_name&amp;quot;        
[64] &amp;quot;place_type&amp;quot;              &amp;quot;country&amp;quot;                 &amp;quot;country_code&amp;quot;           
[67] &amp;quot;geo_coords&amp;quot;              &amp;quot;coords_coords&amp;quot;           &amp;quot;bbox_coords&amp;quot;            
[70] &amp;quot;status_url&amp;quot;              &amp;quot;name&amp;quot;                    &amp;quot;location&amp;quot;               
[73] &amp;quot;description&amp;quot;             &amp;quot;url&amp;quot;                     &amp;quot;protected&amp;quot;              
[76] &amp;quot;followers_count&amp;quot;         &amp;quot;friends_count&amp;quot;           &amp;quot;listed_count&amp;quot;           
[79] &amp;quot;statuses_count&amp;quot;          &amp;quot;favourites_count&amp;quot;        &amp;quot;account_created_at&amp;quot;     
[82] &amp;quot;verified&amp;quot;                &amp;quot;profile_url&amp;quot;             &amp;quot;profile_expanded_url&amp;quot;   
[85] &amp;quot;account_lang&amp;quot;            &amp;quot;profile_banner_url&amp;quot;      &amp;quot;profile_background_url&amp;quot; 
[88] &amp;quot;profile_image_url&amp;quot;      &lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;data-pre-processing.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2.Data pre-processing.&lt;/h4&gt;
&lt;p&gt;This stage involved cleaning up our data by removing the unwanted columns/variables. We decided to do with a select few variables we thought would be most appropriate for our case study. We chose the following &lt;strong&gt;seven variables:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Text&lt;/strong&gt; - This column contained the actual tweets text.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Verified&lt;/strong&gt; - whether or not the user is verified on twitter.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Protected&lt;/strong&gt; - Whether a user is or isn’t protected (Locked accounts).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Location&lt;/strong&gt; - Based on our challenge stated in the &lt;strong&gt;figure above&lt;/strong&gt;, this variable was our most important variable. Rows with &lt;strong&gt;NULL&lt;/strong&gt; values for location simply meant that the specific user &lt;strong&gt;DID NOT GEOTAG&lt;/strong&gt; their tweet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Followers Count&lt;/strong&gt; - Number of followers the user had.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Retweet Verifie&lt;/strong&gt; - Whether the tweet had been retweeted by a verified user or not.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Source&lt;/strong&gt; - Source of the tweet i.e “Android”, “web client” e.t.c&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Code for the data cleanup and variables setting that was done in Python can be found &lt;a href=&#34;https://github.com/Ogutu-Brian/DataCleanup/blob/develop/analysis.py&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;re-importing-data-in-r-and-setting-up-for-the-models&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;2.1 Re-importing Data in R and setting up for the Models&lt;/h4&gt;
&lt;p&gt;After cleaning up the data, we imported it into R, the code chunk shows a preview of the top 4 rows of the input data.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;                                                                                                                                                                    Text
1074                                                                                                                                                            Want to know how to optimize hyper-parameters in Caret with cost-specific functions? #rstats #datascience https://t.co/cupvirSXU9
1316                                                                                                                              via @RichardEudes - Quick Guide to R and Statistical Programming https://t.co/GfyhLMgiuB #analytics, #datascience, #rstats, #statistics https://t.co/Cx3TGJTOoI
2636                                              small #rstats trick: if you need to know if a *sorted* variable is equally spaced (e.g., if it&amp;#39;s a contiguous sequence of ints, which was my use case) you can look at the characteristics of diff(x), e.g.\n\nsummary(diff(x))\ntable(diff(x))
2939 my #ggplot2 flipbook project is online! &amp;lt;U+0001F60E&amp;gt;&amp;lt;U+0001F913&amp;gt;&amp;lt;U+0001F913&amp;gt; Incrementally walks through plotting code (#MakeoverMonday, soon #TidyTuesday plots). Using #xaringan with reveal function; thanks, @statsgen @grrrck. #rstats. https://t.co/bBBzv0iZLw https://t.co/tFtD78IOHZ
     Verified Protected          Location Followers VerifiedRetweet
1074    FALSE     FALSE         Singapore      1570           FALSE
1316    FALSE     FALSE     Paris, France      2151              NA
2636    FALSE     FALSE Pleasant Hill, CA      1207              NA
2939    FALSE     FALSE         Sri Lanka      2623           FALSE
              Characters
1074          DS-retweet
1316               IFTTT
2636  Twitter Web Client
2939 Twitter for Android&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We still had to do some data pre-processing for the models which includes checking for and removing NULL values if present. Below is a sample table of the final data set used in the analysis.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th&gt;Verified&lt;/th&gt;
&lt;th&gt;Protected&lt;/th&gt;
&lt;th&gt;Location&lt;/th&gt;
&lt;th&gt;Followers&lt;/th&gt;
&lt;th&gt;VerifiedRetweet&lt;/th&gt;
&lt;th&gt;Characters&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;TAGGED&lt;/td&gt;
&lt;td&gt;&amp;gt;500&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;55&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;NON-TAGGED&lt;/td&gt;
&lt;td&gt;&amp;gt;500&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;80&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;TAGGED&lt;/td&gt;
&lt;td&gt;&amp;gt;500&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;193&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;TAGGED&lt;/td&gt;
&lt;td&gt;&amp;gt;500&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;188&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;TAGGED&lt;/td&gt;
&lt;td&gt;500&amp;lt;&lt;/td&gt;
&lt;td&gt;YES&lt;/td&gt;
&lt;td&gt;188&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;NON-TAGGED&lt;/td&gt;
&lt;td&gt;500&amp;lt;&lt;/td&gt;
&lt;td&gt;NO&lt;/td&gt;
&lt;td&gt;190&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;From the table above, we can observe a new column “Characters”. This was an additional variable derived by counting the number of characters in the tweet text.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-specifications&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3.Model Specifications&lt;/h4&gt;
&lt;p&gt;Due to the nature of our problem,(we had several uncorrelated variables) we decided to do a classification analysis. This means we had to come up with a classifier model to regress &lt;strong&gt;n&lt;/strong&gt; variables based on our dependent variable, the &lt;strong&gt;Location&lt;/strong&gt; variable. The main challenge of classifier models is knowing what really goes on inside the models that leads to the final output. Even with higher levels of accuracy in some models, it is quite difficult o understand the paths of a given model. However, using &lt;strong&gt;Random forests&lt;/strong&gt; and &lt;strong&gt;Decision Tree&lt;/strong&gt; classifiers can give us a graphical representation of the criteria followed by the models to arrive at a given output. Another upper hand of decision tree models is that they require minimal data cleaning, less time consuming. Here is a detailed read on &lt;a href=&#34;https://medium.com/x8-the-ai-community/decision-trees-an-intuitive-introduction-86c2b39c1a6c&#34;&gt;how decision trees work&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-train-test-sets.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Creating Train &amp;amp; Test Sets.&lt;/h4&gt;
&lt;p&gt;For the training and test data sets, we randomly split our data set into two sates. Usually, the best practice is to train the model with with a larger proportion of the data set. We therefore took &lt;strong&gt;80%&lt;/strong&gt; for training and &lt;strong&gt;20%&lt;/strong&gt; for test purposes.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-training.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Model Training.&lt;/h4&gt;
&lt;p&gt;We trained our decision tree model to predict a &lt;strong&gt;class&lt;/strong&gt; “location”. Whether a location is &lt;strong&gt;geotagged&lt;/strong&gt; or &lt;strong&gt;not geotagged&lt;/strong&gt; based on whether the user is verified, protected, has over 500 followers, is retweeted by another verified user and the number of characters in their tweet. Bellow is the visual output of the trained model.&lt;/p&gt;
&lt;/center&gt;
&lt;figure&gt;
&lt;img class=&#34;pure-u-1-2 pure-u-md-1-3&#34; src=&#34;https://github.com/CarlvinJerry/CarlvinJerry.github.io/blob/master/post/MyImages/tagged.png?raw=true&#34;  height=&#34;100%&#34; width=&#34;Auto&#34;&gt;
&lt;figcaption&gt;
Figure 2: The Tree
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;When interpreting decision trees, you start at the root node. The root node is the one on top of the decision tree. Since what we want is those nodes with geotagged locations, it is safe to ignore the non-tagged nodes. Note that our highest entropy level was observed on one variable only, the number of characters in the tweet text. This might not always be the case with decision trees though, it is possible to have more than one factor. In such situations, it is best to run several decision trees to build a &lt;strong&gt;random forest&lt;/strong&gt; and make a decision based on the most prevalent variables.&lt;br /&gt;
For our case, we only focus on what we found:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;At the top node, we can see the overall probability of a user geotagging their tweets. &lt;strong&gt;75 percent&lt;/strong&gt; of the users in the training set geotagged their tweets. not&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Our second node asks whether the number of characters are &lt;strong&gt;more than 134&lt;/strong&gt; and goes to depth 2 where we can observe the &lt;strong&gt;highest number of users tweeted more than 134 characters&lt;/strong&gt; at &lt;strong&gt;80 percent&lt;/strong&gt; with an &lt;strong&gt;80 percent probability&lt;/strong&gt; of geotagging their tweets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Node 3 checks if the number of characters in a tweet is &lt;strong&gt;less than 134&lt;/strong&gt;. If yes, head to depth 3, where we can see that &lt;strong&gt;20 percent&lt;/strong&gt; of users had less than 134 characters with a &lt;strong&gt;50 percent probability&lt;/strong&gt; of geotagging their tweets.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Finally, looking at depth 4 which originates from the node that checks is number of characters is &lt;strong&gt;equal to or more than 122&lt;/strong&gt;, we can see that &lt;strong&gt;12 percent&lt;/strong&gt; of users had tweets with character equal to or more than 124, with &lt;strong&gt;88 percent probability&lt;/strong&gt; of geotagging their tweets.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-testing-and-performance-accuracy.&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;3.1 Model Testing and performance accuracy.&lt;/h4&gt;
&lt;p&gt;With our model trained and outputs observed, we were able to run a test with our test subset. Here is our confusion matrix.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;confusion-matrix&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Confusion matrix&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;          predict_geotags
             NON-TAGGED TAGGED
  NON-TAGGED         90    248
  TAGGED              2   1043&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;model-accuracy&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Model Accuracy&lt;/h4&gt;
&lt;pre&gt;&lt;code&gt;&amp;gt; #performance
&amp;gt; accuracy_Test &amp;lt;- sum(diag(table_mat)) / sum(table_mat)
&amp;gt; print(paste(&amp;#39;Accuracy for test is&amp;#39;, accuracy_Test))
[1] &amp;quot;Accuracy for test is 0.819233550253073&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;From the confusion matrix above, we can observe that the model had a &lt;strong&gt;true negative of 90 predictions&lt;/strong&gt;. That is, 90 predictions were correctly predicted as not geotagged. A &lt;strong&gt;false positive of 248 predictions&lt;/strong&gt; was observed where the model wrongly predicted 248 tweets were geotagged whereas in real sense they were not.&lt;/p&gt;
&lt;p&gt;For the tagged tweets, we had a &lt;strong&gt;false negative of 2 predictions&lt;/strong&gt; against a &lt;strong&gt;true positive of 1043 predictions&lt;/strong&gt;. This means that our model was able to correctly predict 1043 geotagged tweets from the test data. The accuracy of the model turned out pretty good, at an &lt;strong&gt;82 percent accuracy level&lt;/strong&gt;. The theoretical formula for the accuracy is the proportion of true positives and the true negatives divided by the sum of the confusion matrix.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img class=&#34;pure-u-1-2&#34;  src = &#34;https://github.com/CarlvinJerry/CarlvinJerry.github.io/blob/master/post/MyImages/math.jpg?raw=true&#34;&gt;
&lt;figcaption&gt;
Figure 3: The formula
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;For a better accuracy level, the model’s hyper-parameters can be tweaked to improve performance. Another option is implementing a random forest test.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion-and-recommendation&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;4. Conclusion and Recommendation&lt;/h4&gt;
&lt;p&gt;With our decision tree model, we were able to attain a high level of accuracy for a model that test whether users with &lt;strong&gt;tweets containing characters equal to or above 122&lt;/strong&gt; are likely to geotag their tweets. Our &lt;strong&gt;nudge&lt;/strong&gt; in this case is the number of characters in a tweet and precisely, &lt;strong&gt;124 or more&lt;/strong&gt;. Our recommendation therefore would be to encourage users to tweet longer or engage them in trending topics that require one to write more, for example a TT like # MyLifeHistoryInANutshell…-in the hope that a user will eventually geotag their tweet.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Come to think of it, did twitter really increase the number of characters just for tweeps to tweet more and as they said, to get more people to join twitter? I have a theory, it was a NUDGE!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;References&lt;/h4&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;https://www.businessballs.com/improving-workplace-performance/nudge-theory/&#34;&gt;Business balls official website&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Thaler, R.H., Sunstein, C.R., and Balz, J.P. Choice Architecture. SSRN Electronic Journal (2010), 1–18; &lt;a href=&#34;https://ssrn.com/abstract=1583509&#34; class=&#34;uri&#34;&gt;https://ssrn.com/abstract=1583509&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Thaler, R.H. and Sunstein, C.R. Nudge: Improving Decisions About Health, Wealth, and Happiness&lt;/strong&gt;. Yale University Press, New Haven, CT, and London, U.K., 2008.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;img src=&#34;https://fontmeme.com/permalink/190129/8b378e9ce35b7a28dd150c4f1d656807.png&#34; /&gt;&lt;/p&gt;
&lt;center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
      
            <category>Twitter</category>
      
            <category>Data Mining</category>
      
            <category>R,Python</category>
      
            <category>Decision trees</category>
      
            <category>regression</category>
      
      
            <category>Data Mining</category>
      
            <category>Machine Learning</category>
      
    </item>
    
    <item>
      <title>DATA MINING IN R</title>
      <link>/post/2019-01-25-r-data-mining-rmd/</link>
      <pubDate>Fri, 25 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-01-25-r-data-mining-rmd/</guid>
      <description>


&lt;p&gt;#PartI{#PartI}
&amp;gt;This first section assumes you have no knowledge in building a twitter app to be used for fetching data. You can skip directly to &lt;a href=&#34;#PartII&#34;&gt;the second section here&lt;/a&gt; if you are able to build your own twitter app and get the required authentication keys.&lt;/p&gt;
&lt;div id=&#34;introduction&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Introduction:&lt;/h3&gt;
&lt;p&gt;Social media usage has grown rapidly over the past few decades. Most social networks we can think of now are so well established, making them a platform where people can not only talk to one another but also be able to come forward with different views and interests they would like to express. With an almost constant rate of increasing users each day, social networks such as Facebook and Twitter have become great sources of data which can be used in the broad field of Data Science:Talk of (those pretty annoying) targeted ads for example…&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://media.giphy.com/media/3o6Mbmg6AchRmB4ylO/giphy.gif&#34; alt=&#34;Gif: They just keep coming!!&#34; /&gt;
&lt;center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;With the help of APIs, we are easily able to get data from such platforms to be used for further analysis. In this article we will go through the first step of text mining in R using data fetched from Twitter. The main advantage of these APIs is that the data we will fetch comes in a well-structured format which makes our work easier when crunching. In this case we will use the readily available Twitter API and create our own Twitter app that will then help in fetching the data. The following steps will accomplish the task…&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;creating-a-twitter-app&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Creating a Twitter app&lt;/h3&gt;
&lt;p&gt;To create a twitter app we can use for fetching metadata, we first need to have a Twitter account. We then need to go to the &lt;strong&gt;twitter dev site&lt;/strong&gt; &lt;a href=&#34;https://developer.twitter.com/&#34; class=&#34;uri&#34;&gt;https://developer.twitter.com/&lt;/a&gt; and log in with our user account.
On the top right corner should be a drop down menu next to your username, go to APPS. At this point if you are doing this for the first time your Apps section should be blank. Click on “Create an app” to… create an app.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://github.com/CarlvinJerry/carlvinjerry.github.io/blob/master/post/MyImages/apps.PNG?raw=true&#34; alt=&#34;Figure 1: App login&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;We then have to fill in the form below appropriately. Here is a breakdown of what’s required:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Name:&lt;/strong&gt;
Give your app a unique name of your choice, e.g &lt;strong&gt;UniqueName&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Description:&lt;/strong&gt;
This can always be changed later, use this to provide a brief note on what your app is all about to be able to distinguish it from other apps you might create in future.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Website:&lt;/strong&gt;
This should be your application’s home page web site. It is however not applicable for most personal apps. Anything goes here e.g &lt;a href=&#34;https://carlvinjerry.github.io&#34; class=&#34;uri&#34;&gt;https://carlvinjerry.github.io&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Callback URL:&lt;/strong&gt;
I would ignore the Callback URL field. If you are allowing users to log into your app to authenticate themselves, you’d enter the URL where they would be returned after they’ve given permission to Twitter to use your app.&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;img src=&#34;https://github.com/CarlvinJerry/CarlvinJerry.github.io/blob/master/post/MyImages/apps2.PNG?raw=true&#34; alt=&#34;Figure 2: Create an app&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The remaining fields should be quite straight forward but must be filled. Click “Create” once done and there you have your first twitter app.
On your app is a menu with &lt;strong&gt;Keys and tokens&lt;/strong&gt;. These are the most important components since we will need them to access data from the APIs.Generate both consumer and access tokens (if not readily available) and take note of them. &lt;strong&gt;NB:&lt;/strong&gt; &lt;strong&gt;These keys are meant for your eyes only!&lt;/strong&gt;&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://github.com/CarlvinJerry/CarlvinJerry.github.io/blob/master/post/MyImages/keys.PNG?raw=true&#34; alt=&#34;Figure 3: API Keys&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;The final bit of setting up our twitter app is granting access permissions. We will mostly do fine with the &lt;strong&gt;read-only&lt;/strong&gt; if all we need is to fetch data but it can always be changed later.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://github.com/CarlvinJerry/CarlvinJerry.github.io/blob/master/post/MyImages/permisions.PNG?raw=true&#34; alt=&#34;Figure 4: Permissions&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Now we can move on to the next step where we set up &lt;strong&gt;R&lt;/strong&gt; to query data from Twitter.&lt;/p&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;hr /&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;#PartII{#PartII}
## Setting up R to fetch twitter data
With our twitter app set up &lt;a href=&#34;#PartI&#34;&gt;in part I above&lt;/a&gt; and we are able to get the authentication keys for the API, we can now easily fetch data from twitter in R. The following steps will help us do this:&lt;/p&gt;
&lt;div id=&#34;prerequisites&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Prerequisites:&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Twitter API Keys:&lt;/strong&gt; At this point we already have our twitter app with the required API keys.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;R and an IDE of choice:&lt;/strong&gt; We also need to have R installed, advisably the latest version. Microsoft’s &lt;a href=&#34;https://mran.microsoft.com/open&#34;&gt;&lt;strong&gt;enhanced R distribution&lt;/strong&gt;&lt;/a&gt; is recommended over the
&lt;a href=&#34;https://cran.r-project.org/bin/windows/base/&#34;&gt;&lt;strong&gt;base R&lt;/strong&gt;&lt;/a&gt; but for this specific task either can do just fine. I would recommend &lt;a href=&#34;https://www.rstudio.com/products/rstudio/download/&#34;&gt;&lt;strong&gt;R STUDIO&lt;/strong&gt;&lt;/a&gt; for an &lt;strong&gt;IDE&lt;/strong&gt;. One obvious advantage of all these is that they’re open-source tools.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;install-and-load-the-required-packages-in-r&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;1.Install and Load the required packages in R&lt;/h3&gt;
&lt;p&gt;R has a standard set of packages, each with different tasks. You can find some packages for download&lt;br /&gt;
&lt;a href=&#34;https://cran.cnr.berkeley.edu/&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;. The code chunk below installs and loads the specific packages we need for this task. Take note of comments at each line of code, initiated by an octothorp.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Install packages
install.packages(&amp;quot;twitteR&amp;quot;)#------Extracts data from twitter
install.packages(&amp;quot;httr&amp;quot;)#--------Tools for Working with URLs and HTTP&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#We can now load the two packages 
library(&amp;quot;twitteR&amp;quot;)
require(&amp;quot;httr&amp;quot;)#-------------Both require() and library() can be used to call an installed package&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;NB:&lt;/strong&gt; Windows users might need to download a certification file and store it in the working directory. This certificate file initiates a handshake between R and the Twitter API.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Download &amp;quot;cacert.pem&amp;quot; file
download.file(url=&amp;quot;https://curl.haxx.se/ca/cacert.pem&amp;quot;,destfile=&amp;quot;cacert.pem&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;create-and-store-objects-containing-the-twitter-authenticated-credentials&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;2.Create and store objects containing the twitter authenticated credentials&lt;/h3&gt;
&lt;p&gt;This is where we invoke the twitter API using the credentials from our app and query the data we need.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Authentication keys
consumer_key &amp;lt;- &amp;#39;hjksdha08097afnjhaa90uaf&amp;#39;
consumer_secret &amp;lt;- &amp;#39;hjksdha08097afnjhaa90uaf&amp;#39;
access_token &amp;lt;- &amp;#39;hjksdha08097afnjhaa90uaf&amp;#39;
access_secret &amp;lt;- &amp;#39;hjksdha08097afnjhaa90uaf&amp;#39;

#The above tokens are what we made the twitter app for.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;br&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;query-data-from-twitter&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;3.Query data from twitter&lt;/h3&gt;
&lt;p&gt;We can now go ahead and fetch our data. Due to limitations on the twitter standard apps, it is advisable to store your data in R locally. This will reduce the number of times you have to make requests to twitter to fetch data. You will therefore do much more with your app that way regardless of the limitations- Or you can as well buy the premium app. In my example below, I am fetching data for a user on twitter called &lt;span class=&#34;citation&#34;&gt;@UKenyatta&lt;/span&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#Connect to Twitter 
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)
tweets &amp;lt;- userTimeline(&amp;quot;UKenyatta&amp;quot;, n=3200) #Standard twitter apps are limited to 3200 tweets per                                                  #download session. This could come out less depending on
                                            #the app

#create a data frame of the tweets
UKenyatta_Tweets &amp;lt;- tbl_df(map_df(tweets, as.data.frame))

# Save tweets for later (and note when saved):
save(UKenyatta_Tweets, file=&amp;quot;UKenyatta_Tweets.RData&amp;quot;)

# You can then access them later at will...
# load(&amp;quot;UKenyatta_Tweets.RData&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;You can now manipulate your data and see what you find out.&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://media.giphy.com/media/ZtMkorgeyRu5q/giphy.gif&#34; alt=&#34;Happy crunching!!&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;center&gt;
&lt;img src=&#34;https://fontmeme.com/permalink/190129/8b378e9ce35b7a28dd150c4f1d656807.png&#34; /&gt;
&lt;/center&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
</description>
      
            <category>Twitter</category>
      
            <category>Data Mining</category>
      
            <category>R</category>
      
            <category>Data Visualization</category>
      
            <category>Sentiment Analysis</category>
      
      
            <category>Data Mining</category>
      
            <category>R</category>
      
    </item>
    
  </channel>
</rss>